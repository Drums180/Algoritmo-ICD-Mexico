# Función para detectar las flags en la columna ImageQuality
detect_flags <- function(quality_string) {
quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
detected <- quality_values[quality_values %in% flag_values]
if(length(detected) > 0) {
return(paste(detected, collapse = ","))
} else {
return(NA_character_)
}
}
# Función modificada para comprobar si alguno de los valores de flag está en la columna ImageQuality
check_flags <- function(quality_string) {
quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
if(any(quality_values %in% flag_values)) {
# Verificar si el valor 15 está en la lista de calidad
if(15 %in% quality_values) {
return(-20)
}
# Verificar si el valor 14 está en la lista de calidad
if(14 %in% quality_values) {
return(-20)
}
# Si no se cumple ninguna de las condiciones anteriores
return(-100)
} else {
# Si no hay valores de calidad en la lista de banderas
return(100)
}
}
# Aplicar la función modificada a la columna ImageQuality de result_scenes
result_scenes$flag_trigger <- sapply(result_scenes$ImageQuality, check_flags)
result_scenes$detected_flags <- sapply(result_scenes$ImageQuality, detect_flags)
# Verificar el resultado
head(result_scenes)
master_clientes <- master_clientes %>%
mutate(
cuota_diaria = 5
)
# Verificar el resultado
head(master_clientes)
master_calidad <- df_survey %>%
select(SessionUID = "Session Uid", SurveyType = "Survey Type", User, `Outlet Code`, duration, `Survey End Time`, estatus) %>%
# Unión con master_clientes
left_join(master_clientes %>%
mutate(codigo = as.character(codigo)) %>%
select(codigo, tamano, agenciabi, region, actividad, subcanal, modeloservicio, zona, rutaventa, cuota_diaria),
by = c("Outlet Code" = "codigo")) %>%
# Incorporar columnas de df_session
left_join(df_session %>% select(SessionUId, distance),
by = c("SessionUID" = "SessionUId")) %>%
# Incorporar métricas de frentes_df
left_join(frentes_df %>% select(SessionUID, num_frentes, frentes_toni),
by = "SessionUID") %>%
# Incorporar columna flag_trigger de result_scenes
left_join(result_scenes %>% select(SessionUID, flag_trigger, detected_flags),
by = "SessionUID") %>%
# Seleccionar y reordenar columnas
select(
SessionUID, SurveyType, User, `Outlet Code`,
agenciabi, tamano, region, actividad, subcanal,
duration, distance,
num_frentes, frentes_toni, flag_trigger, detected_flags,
modeloservicio, zona, rutaventa, `Survey End Time`, estatus, cuota_diaria
)
# Rellenar valores de cuota
master_calidad <- master_calidad %>%
mutate(
cuota_diaria = 5
)
# Añadir la información de df_scenenum a master_calidad
master_calidad <- master_calidad %>%
left_join(df_scenenum, by = "SessionUID")
# Paso 1: Calcular cuántos registros duplicados hay por SessionUID
duplicates_count <- master_calidad %>%
group_by(SessionUID) %>%
summarise(dup_count = n()) %>%
filter(dup_count > 1)
# Paso 2: Actualizar master_calidad con los valores ajustados
master_calidad <- master_calidad %>%
left_join(duplicates_count, by = "SessionUID") %>%
mutate(
num_frentes = ifelse(!is.na(dup_count), num_frentes/dup_count, num_frentes),
frentes_toni = ifelse(!is.na(dup_count), frentes_toni/dup_count, frentes_toni),
Num.Scenes = ifelse(!is.na(dup_count), `Num.Scenes`/dup_count, `Num.Scenes`),
dup_count = ifelse(is.na(dup_count), 1, dup_count)
) %>%
select(-dup_count) # Eliminar la columna temporal
# Paso 3: Eliminar registros duplicados
master_calidad <- master_calidad %>%
distinct(SessionUID, .keep_all = TRUE)
# Verificar el nuevo dataframe
head(master_calidad)
# 1. Contar los SceneUID distintos para cada SessionUID
scene_count_df <- df_actual %>%
group_by(SessionUID) %>%
summarise(min_frentes = n_distinct(SceneUID) * 3)
# Unir temporalmente con master_calidad para aplicar las condiciones
master_calidad_temp <- left_join(master_calidad, scene_count_df, by = "SessionUID")
# Actualizar las flags de acuerdo con las condiciones
master_calidad <- master_calidad_temp %>%
mutate(
# Flag para 0 frentes totales
detected_flags = ifelse(num_frentes == 0,
ifelse(is.na(detected_flags), "66", paste(detected_flags, ",66", sep="")),
detected_flags),
flag_trigger = ifelse(num_frentes == 0, -100, flag_trigger),
# Flag para frentes insuficientes
detected_flags = ifelse(num_frentes < min_frentes,
ifelse(is.na(detected_flags), "61", paste(detected_flags, ",61", sep="")),
detected_flags),
flag_trigger = ifelse(num_frentes < min_frentes, -100, flag_trigger)
# Eliminado el bloque de código que maneja la flag de enfriadores
) %>%
select(-min_frentes)  # Eliminamos la columna temporal min_frentes
head(master_calidad)
# Asumiendo que 'df_actual' tiene los datos de SKUs y 'df_scenes' tiene los datos de SceneType.
# También asumimos que ambos dataframes pueden ser unidos por SceneUID.
# Paso 1: Unir df_actual con df_scenes para tener el SceneType disponible
df_actual_with_type <- left_join(df_actual, df_scenes %>%
select(SessionUID, SceneType), by = "SessionUID")
# Paso 2: Crear tokens basados en los SKUs dentro de cada SceneUID
# excluyendo los tokens con 5 SKUs idénticos, que sean "0-0-0-0-0", "NA-NA-NA-NA-NA" o con 3 o más ceros.
df_tokens <- df_actual_with_type %>%
arrange(SessionUID, SceneUID, Shelf, Position, StockPos) %>%
group_by(SessionUID, SceneUID) %>%
mutate(token_index = ceiling(row_number() / 5)) %>%
group_by(SessionUID, SceneUID, token_index) %>%
summarise(token = paste(SKU, collapse = "-"), .groups = "drop") %>%
mutate(
all_skus_same = n_distinct(strsplit(token, "-")[[1]]) == 1,
all_skus_zero_or_na = token %in% c("0-0-0-0-0", "NA-NA-NA-NA-NA"),
# Añadir condición para contar la cantidad de ceros en el token
num_zeros = str_count(token, "0"),
num_na = str_count(token, "NA"),
# Añadir una lógica para excluir tokens con 3 o más ceros
exclude_token = all_skus_zero_or_na | num_zeros >= 3 | num_na >= 3
) %>%
filter(!all_skus_same, !exclude_token) %>%
left_join(df_scenes %>% select(SceneUID, SceneType), by = "SceneUID")
# Paso 3: Identificar tokens duplicados dentro de la misma SessionUID y diferentes SceneUID
# que tengan el mismo SceneType y que existan al menos 3 tokens idénticos.
df_duplicated_tokens <- df_tokens %>%
group_by(SessionUID, token, SceneType) %>%
mutate(token_count = n_distinct(SceneUID)) %>%
filter(token_count >= 4) %>%
distinct(SessionUID, .keep_all = TRUE)
# Paso 4: Crear una lista de SessionUID con al menos 3 tokens duplicados en SceneUID distintos y mismo SceneType
sessions_with_duplicates <- unique(df_duplicated_tokens$SessionUID)
# Paso 5: Actualizar el DataFrame master_calidad con la nueva flag y ajustar flag_trigger
master_calidad <- master_calidad %>%
mutate(
detected_flags = ifelse(SessionUID %in% sessions_with_duplicates,
ifelse(is.na(detected_flags), "77", paste(detected_flags, ",77", sep="")),
detected_flags),
flag_trigger = ifelse(SessionUID %in% sessions_with_duplicates, -100, flag_trigger)
)
master_calidad <- master_calidad %>%
mutate(
`Survey End Time` = as.Date(substr(`Survey End Time`, 1, 10), format = "%d/%m/%Y")
)
# Verificar el cambio en la columna
head(master_calidad)
# Paso 1: Unión con parametros_data
master_evaluado <- master_calidad %>%
left_join(parametros_data, by = c("tamano", "subcanal"))
# Paso 2: Generar subconjuntos basados en combinaciones de tamaño y subcanal "General"
general_micro <- parametros_data[parametros_data$tamano == "1 - MICRO" & parametros_data$subcanal == "General", ]
general_chico <- parametros_data[parametros_data$tamano == "2 - CHICO" & parametros_data$subcanal == "General", ]
general_mediano <- parametros_data[parametros_data$tamano == "3 - MEDIANO" & parametros_data$subcanal == "General", ]
general_grande <- parametros_data[parametros_data$tamano == "4 - GRANDE" & parametros_data$subcanal == "General", ]
general_extragrande <- parametros_data[parametros_data$tamano == "5 - EXTRAGRANDE" & parametros_data$subcanal == "General", ]
# Paso 2.1: Eliminar filas con NA en cada subconjunto
general_micro <- na.omit(general_micro)
general_chico <- na.omit(general_chico)
general_mediano <- na.omit(general_mediano)
general_grande <- na.omit(general_grande)
general_extragrande <- na.omit(general_extragrande)
# Paso 3: Iterar sobre columnas específicas para reemplazar NA
cols_to_modify <- c("lower_bound_frentes", "upper_bound_frentes",
"lower_bound_duration", "upper_bound_duration")
for(col in cols_to_modify) {
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "1 - MICRO", general_micro[[col]], master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "2 - CHICO", general_chico[[col]], master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "3 - MEDIANO", general_mediano[[col]],  master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "4 - GRANDE", general_grande[[col]],  master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "5 - EXTRAGRANDE", general_extragrande[[col]], master_evaluado[[col]])
# Agregar esta línea para manejar el caso donde 'tamano' es NA
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & is.na(master_evaluado$tamano), general_mediano[[col]], master_evaluado[[col]])
}
# Imputación final para reemplazar cualquier NA restante con los valores de 'tradicional_M'
for(col in cols_to_modify) {
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]), general_mediano[[col]], master_evaluado[[col]])
}
# Paso 4: Calificación
master_evaluado <- master_evaluado %>%
mutate(
margin = (upper_bound_frentes - lower_bound_frentes) * 0.1,
score_frentes = case_when(
num_frentes >= lower_bound_frentes & num_frentes <= upper_bound_frentes ~ 100,
(num_frentes < lower_bound_frentes & num_frentes >= (lower_bound_frentes - margin)) |
(num_frentes > upper_bound_frentes & num_frentes <= (upper_bound_frentes + margin)) ~ 50,
num_frentes < lower_bound_frentes - margin | num_frentes > upper_bound_frentes + margin ~ 0,
TRUE ~ NA_real_
),
score_duration = case_when(
duration >= lower_bound_duration & duration <= upper_bound_duration ~ 100,
duration < lower_bound_duration | duration > upper_bound_duration ~ 0,
TRUE ~ NA_real_
)
)
# Paso 5: Evaluación Scenes
master_evaluado <- master_evaluado %>%
mutate(
score_scenes = case_when(
(tamano == "1 - MICRO" & Num.Scenes == 2) ~ 10,
(tamano == "1 - MICRO" & Num.Scenes == 3) ~ 20,
(tamano == "1 - MICRO" & Num.Scenes > 2 & Num.Scenes < 3) ~ 15,
(tamano == "1 - MICRO" & (Num.Scenes < 2 | Num.Scenes > 3)) ~ 0,
(tamano == "2 - CHICO" & Num.Scenes == 2) ~ 10,
(tamano == "2 - CHICO" & Num.Scenes == 4) ~ 20,
(tamano == "2 - CHICO" & Num.Scenes > 2 & Num.Scenes < 4) ~ 15,
(tamano == "2 - CHICO" & (Num.Scenes < 2 | Num.Scenes > 4)) ~ 0,
(tamano == "3 - MEDIANO" & Num.Scenes == 2) ~ 10,
(tamano == "3 - MEDIANO" & Num.Scenes == 6) ~ 20,
(tamano == "3 - MEDIANO" & Num.Scenes > 2 & Num.Scenes < 6) ~ 15,
(tamano == "3 - MEDIANO" & (Num.Scenes < 2 | Num.Scenes > 6)) ~ 0,
(tamano == "4 - GRANDE" & Num.Scenes == 3) ~ 10,
(tamano == "4 - GRANDE" & Num.Scenes == 7) ~ 20,
(tamano == "4 - GRANDE" & Num.Scenes > 3 & Num.Scenes < 7) ~ 15,
(tamano == "4 - GRANDE" & (Num.Scenes < 3 | Num.Scenes > 7)) ~ 0,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes == 4) ~ 10,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes == 8) ~ 20,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes > 4 & Num.Scenes < 8) ~ 15,
(tamano == "5 - EXTRAGRANDE" & (Num.Scenes < 4 | Num.Scenes > 8)) ~ 0,
TRUE ~ NA_real_
)
)
# Verificar el resultado
summary(master_evaluado)
# Guardar archivo
write.xlsx(master_evaluado, "master_calidad.xlsx")
# Calcular promedios y cuantiles por 'tamano' para 'duration' y 'frentes_toni'
promedios_por_tamano <- master_calidad %>%
group_by(tamano) %>%
summarise(
promedio_duration = mean(duration, na.rm = TRUE),
lower_bound_duration = quantile(duration, 0.35, na.rm = TRUE),
upper_bound_duration = quantile(duration, 0.8, na.rm = TRUE) * 3,
promedio_frentes_toni = mean(frentes_toni, na.rm = TRUE),
lower_bound_frentes_toni = quantile(frentes_toni, 0.5, na.rm = TRUE),
upper_bound_frentes_toni = quantile(frentes_toni, 0.8, na.rm = TRUE) * 7,
.groups = 'drop'
)
# Crear filas "Generales" con la data anterior
general_por_tamano <- promedios_por_tamano %>%
mutate(subcanal = "General") %>%
rename(
lower_bound_duration = lower_bound_duration,
upper_bound_duration = upper_bound_duration,
lower_bound_frentes = lower_bound_frentes_toni,
upper_bound_frentes = upper_bound_frentes_toni
)
# Eliminar filas con 'tamano' = NA
parametros_unificados <- parametros_unificados %>%
filter(!is.na(tamano) & tamano != "NA")
# Calcular límites por tamaño usando IQR
master_calidad %>%
group_by(tamano) %>%
summarise(
Q1 = quantile(duration, 0.25, na.rm = TRUE),
Q3 = quantile(duration, 0.75, na.rm = TRUE)
) %>%
mutate(
IQR = Q3 - Q1,
lower_bound = Q1,
upper_bound = Q3 + (1.5 * IQR)
) -> bounds_by_tamano
# Mostrando los resultados
print(bounds_by_tamano)
library(readxl)
library(dplyr)
library(purrr)
# Especifica el nombre del archivo o el patrón del archivo que sabes que es el correcto
file_path <- "fuentes_datos/reporte_metricas/Reporte Operativo HTC Super Liga - Puntos 1-30SEP.xlsx"
# Importa el archivo de Excel directamente en un dataframe
df_from_excel <- readxl::read_excel(file_path)
# Paso 2: Filtrar las sesiones con TOTAL PUNTOS > 40
sessions_to_include <- df_from_excel %>%
filter(`TOTAL PUNTOS` > 40) %>%
select(`SESSION UID`) %>%
distinct()
# Crear un nuevo dataframe a partir de master_calidad que incluye solo las sesiones filtradas
master_calidad_filtered <- master_calidad %>%
semi_join(sessions_to_include, by = c("SessionUID" = "SESSION UID"))
# Paso 3: Aplicar el análisis de IQR para frentes_toni en el nuevo dataframe filtrado
master_calidad_filtered %>%
group_by(tamano, subcanal) %>%
summarise(
Q1 = quantile(frentes_toni, 0.25, na.rm = TRUE),
Q3 = quantile(frentes_toni, 0.75, na.rm = TRUE),
.groups = 'drop'
) %>%
mutate(
IQR = Q3 - Q1,
# Aplica la condición aquí
Q3_adjusted = ifelse(IQR == 0, Q3 * 4, Q3),
lower_bound = Q1,
upper_bound = ifelse(IQR == 0, Q3_adjusted + (8 + IQR + 50), Q3 + (8 * IQR + 50))
) -> bounds_by_tamano_subcanal
# Mostrando los resultados
print(bounds_by_tamano_subcanal)
# Unir los dataframes por 'tamano' para combinar los límites de 'duration' y 'frentes'
parametros_unificados <- full_join(bounds_by_tamano, bounds_by_tamano_subcanal, by = "tamano") %>%
select(
subcanal,
tamano,
lower_bound_duration = lower_bound.x,
upper_bound_duration = upper_bound.x,
lower_bound_frentes = lower_bound.y,
upper_bound_frentes = upper_bound.y
)
# Mostrando los resultados
print(parametros_unificados)
# Calcular promedios y cuantiles por 'tamano' para 'duration' y 'frentes_toni'
promedios_por_tamano <- master_calidad %>%
group_by(tamano) %>%
summarise(
promedio_duration = mean(duration, na.rm = TRUE),
lower_bound_duration = quantile(duration, 0.35, na.rm = TRUE),
upper_bound_duration = quantile(duration, 0.8, na.rm = TRUE) * 3,
promedio_frentes_toni = mean(frentes_toni, na.rm = TRUE),
lower_bound_frentes_toni = quantile(frentes_toni, 0.5, na.rm = TRUE),
upper_bound_frentes_toni = quantile(frentes_toni, 0.8, na.rm = TRUE) * 7,
.groups = 'drop'
)
# Crear filas "Generales" con la data anterior
general_por_tamano <- promedios_por_tamano %>%
mutate(subcanal = "General") %>%
rename(
lower_bound_duration = lower_bound_duration,
upper_bound_duration = upper_bound_duration,
lower_bound_frentes = lower_bound_frentes_toni,
upper_bound_frentes = upper_bound_frentes_toni
)
# Eliminar filas con 'tamano' = NA
parametros_unificados <- parametros_unificados %>%
filter(!is.na(tamano) & tamano != "NA")
# Asegurarse de que 'general_por_tamano' tenga las mismas columnas que 'parametros_unificados', en el mismo orden
general_por_tamano <- general_por_tamano %>%
select(names(parametros_unificados))
# Añadir las filas de 'general_por_tamano' al final de 'parametros_unificados'
parametros_unificados <- bind_rows(parametros_unificados, general_por_tamano)
# Mostrando los resultados
print(parametros_unificados)
# Ruta del archivo de destino
ruta_destino <- "parametros.xlsx"
# Crear un nuevo archivo de Excel con los datos
wb <- createWorkbook()
# Añadir una hoja de cálculo y escribir los datos en ella
addWorksheet(wb, "data")
writeData(wb, sheet = "data", x = parametros_unificados, startRow = 1, startCol = 1, colNames = TRUE)
# Guardar el archivo de Excel
saveWorkbook(wb, ruta_destino, overwrite = TRUE)
# Mensaje de confirmación
cat("Datos exportados exitosamente a", ruta_destino)
#---- PARAMETROS POST EVALUACIÓN ---- (pendientes)
# Cargar la hoja "data" del archivo "parametros"
parametros_data <- read_excel("parametros.xlsx", sheet = "data")
# Verificar la carga de datos
head(parametros_data)
# Paso 1: Unión con parametros_data
master_evaluado <- master_calidad %>%
left_join(parametros_data, by = c("tamano", "subcanal"))
# Paso 2: Generar subconjuntos basados en combinaciones de tamaño y subcanal "General"
general_micro <- parametros_data[parametros_data$tamano == "1 - MICRO" & parametros_data$subcanal == "General", ]
general_chico <- parametros_data[parametros_data$tamano == "2 - CHICO" & parametros_data$subcanal == "General", ]
general_mediano <- parametros_data[parametros_data$tamano == "3 - MEDIANO" & parametros_data$subcanal == "General", ]
general_grande <- parametros_data[parametros_data$tamano == "4 - GRANDE" & parametros_data$subcanal == "General", ]
general_extragrande <- parametros_data[parametros_data$tamano == "5 - EXTRAGRANDE" & parametros_data$subcanal == "General", ]
# Paso 2.1: Eliminar filas con NA en cada subconjunto
general_micro <- na.omit(general_micro)
general_chico <- na.omit(general_chico)
general_mediano <- na.omit(general_mediano)
general_grande <- na.omit(general_grande)
general_extragrande <- na.omit(general_extragrande)
# Paso 3: Iterar sobre columnas específicas para reemplazar NA
cols_to_modify <- c("lower_bound_frentes", "upper_bound_frentes",
"lower_bound_duration", "upper_bound_duration")
for(col in cols_to_modify) {
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "1 - MICRO", general_micro[[col]], master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "2 - CHICO", general_chico[[col]], master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "3 - MEDIANO", general_mediano[[col]],  master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "4 - GRANDE", general_grande[[col]],  master_evaluado[[col]])
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & master_evaluado$tamano == "5 - EXTRAGRANDE", general_extragrande[[col]], master_evaluado[[col]])
# Agregar esta línea para manejar el caso donde 'tamano' es NA
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]) & is.na(master_evaluado$tamano), general_mediano[[col]], master_evaluado[[col]])
}
# Imputación final para reemplazar cualquier NA restante con los valores de 'tradicional_M'
for(col in cols_to_modify) {
master_evaluado[[col]] <- ifelse(is.na(master_evaluado[[col]]), general_mediano[[col]], master_evaluado[[col]])
}
# Paso 4: Calificación
master_evaluado <- master_evaluado %>%
mutate(
margin = (upper_bound_frentes - lower_bound_frentes) * 0.1,
score_frentes = case_when(
num_frentes >= lower_bound_frentes & num_frentes <= upper_bound_frentes ~ 100,
(num_frentes < lower_bound_frentes & num_frentes >= (lower_bound_frentes - margin)) |
(num_frentes > upper_bound_frentes & num_frentes <= (upper_bound_frentes + margin)) ~ 50,
num_frentes < lower_bound_frentes - margin | num_frentes > upper_bound_frentes + margin ~ 0,
TRUE ~ NA_real_
),
score_duration = case_when(
duration >= lower_bound_duration & duration <= upper_bound_duration ~ 100,
duration < lower_bound_duration | duration > upper_bound_duration ~ 0,
TRUE ~ NA_real_
)
)
# Paso 5: Evaluación Scenes
master_evaluado <- master_evaluado %>%
mutate(
score_scenes = case_when(
(tamano == "1 - MICRO" & Num.Scenes == 2) ~ 10,
(tamano == "1 - MICRO" & Num.Scenes == 3) ~ 20,
(tamano == "1 - MICRO" & Num.Scenes > 2 & Num.Scenes < 3) ~ 15,
(tamano == "1 - MICRO" & (Num.Scenes < 2 | Num.Scenes > 3)) ~ 0,
(tamano == "2 - CHICO" & Num.Scenes == 2) ~ 10,
(tamano == "2 - CHICO" & Num.Scenes == 4) ~ 20,
(tamano == "2 - CHICO" & Num.Scenes > 2 & Num.Scenes < 4) ~ 15,
(tamano == "2 - CHICO" & (Num.Scenes < 2 | Num.Scenes > 4)) ~ 0,
(tamano == "3 - MEDIANO" & Num.Scenes == 2) ~ 10,
(tamano == "3 - MEDIANO" & Num.Scenes == 6) ~ 20,
(tamano == "3 - MEDIANO" & Num.Scenes > 2 & Num.Scenes < 6) ~ 15,
(tamano == "3 - MEDIANO" & (Num.Scenes < 2 | Num.Scenes > 6)) ~ 0,
(tamano == "4 - GRANDE" & Num.Scenes == 3) ~ 10,
(tamano == "4 - GRANDE" & Num.Scenes == 7) ~ 20,
(tamano == "4 - GRANDE" & Num.Scenes > 3 & Num.Scenes < 7) ~ 15,
(tamano == "4 - GRANDE" & (Num.Scenes < 3 | Num.Scenes > 7)) ~ 0,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes == 4) ~ 10,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes == 8) ~ 20,
(tamano == "5 - EXTRAGRANDE" & Num.Scenes > 4 & Num.Scenes < 8) ~ 15,
(tamano == "5 - EXTRAGRANDE" & (Num.Scenes < 4 | Num.Scenes > 8)) ~ 0,
TRUE ~ NA_real_
)
)
# Verificar el resultado
summary(master_evaluado)
# Guardar archivo
write.xlsx(master_evaluado, "master_calidad.xlsx")
# Guardar archivo
write.xlsx(master_evaluado, "master_calidad.xlsx")
library(dplyr)
library(readr)
library(readxl)
library(tidyverse)
library(fs) # para funciones de sistema de archivos
library(purrr)
library(openxlsx)
setwd("fuentes_datos/")  # Establecer ruta de trabajo
# Función para leer y combinar archivos de una carpeta
combine_files <- function(path, file_type = c("csv", "excel")) {
# Obtener lista de archivos en el directorio especificado
files <- list.files(path, full.names = TRUE)
df_combined <- data.frame()
process_file <- function(file) {
tryCatch({
if (grepl("\\.csv$", file)) {
data <- read_csv(file)
} else if (grepl("\\.(xlsx|xls|XLSX|XLS)$", file)) {
data <- read_excel(file)
} else {
stop("Tipo de archivo no soportado")
}
# Lista de columnas que deben ser double
cols_to_double <- c("Duration(Sec)", "longitude", "latitude", "SessionEndLongitude", "SessionEndLatitude")
# Convertir todas las columnas a character, excepto las especificadas
data[] <- lapply(names(data), function(col) {
if (col %in% cols_to_double) {
as.numeric(as.character(data[[col]]))
} else {
as.character(data[[col]])
}
})
return(data)
}, error = function(e) {
print(paste("Error al procesar el archivo:", file))  # Imprimir el archivo con error
print(e)  # Imprimir el error específico
return(NULL)
})
}
if ("csv" %in% file_type) {
files_csv <- grep("\\.csv$", files, value = TRUE)
if(length(files_csv) > 0){
df_list_csv <- lapply(files_csv, process_file)
df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_csv))
}
}
if ("excel" %in% file_type) {
files_xls <- grep("\\.(xlsx|xls|XLSX|XLS)$", files, value = TRUE)
if(length(files_xls) > 0){
df_list_xls <- lapply(files_xls, process_file)
df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_xls))
}
}
# Eliminar filas duplicadas
if(nrow(df_combined) > 0) {
df_combined <- df_combined %>% distinct()
}
return(df_combined)
}
# Crear data frames para cada carpeta
df_actual <- combine_files("actual/")
